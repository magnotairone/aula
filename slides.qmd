---
title: "Modelos lineares com regularização para regressão"
author: "Magno Severino"
title-slide-attributes:
  data-background-color: "#002060"
  data-visibility: hidden
format: 
  revealjs:
    theme: simple
    transition: slide
    background-transition: fade
    css: style_slides.css
    width: 1600
    height: 900
    slide-number: true
engine: knitr
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r}
library(tidyverse)
cor = "#002060"

tema <- theme_classic() +
  theme(
    text = element_text(size = 14),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14)
  )

theme_set(tema)
```

# Modelos lineares com regularização para regressão {background-color="#002060"}

**Magno Severino**

## Objetivos de aprendizagem

<!-- ## Motivação (COLOCAR MEU EXEMPLO) -->

<!-- Considere que o objetivo é ajustar um modelo de regressão polinomial a um conjunto de 10 pontos gerados por meio da -->
<!-- expressão $y_i = sen(2\pi x_i)+e_i$, em que $e_i~N(0, \sigma^2.$ -->
<!-- Os dados estão representados na abaixo por pontos em azul.  -->
<!-- A curva verde corresponde a $y_i = sen(2\pi x_i)$; em vermelho estão representados os ajustes baseados em regressões polinomiais de graus, 0, 1, 3 e 9. -->

<!-- ```{r fig.height=4, fig.align='center'} -->
<!-- knitr::include_graphics("figura11.1.png") -->
<!-- ``` -->

<!-- ::: footer -->
<!-- Figura do livro Morettin, Pedro A., and Julio M. Singer. "Introduçao a Ciência de Dados." Introdução à Ciência de Dados: Fundamentos e Aplicações (2020). -->
<!-- ::: -->

## Motivação

Suponha que queremos estimar a função que relaciona o total de anos de estudo ($X$) de uma pessoa com o seu salário anual ($Y$).

Assuma que o **modelo verdadeiro** que relaciona anos de estudo ($X$) e salário ($Y$) seja:
$$
Y = g(X) + \varepsilon,
$$
onde:

- $g(X) = 45 \tanh(x/1.9-7)+57$,
- $\varepsilon\sim N(0, \sigma^2).$

Foram geradas 100 amostras de tamanho $n=30$ e foram ajustados modelos polinomiais de diferentes graus.

## Motivação

Comparativo de modelos com polinômios de diferentes graus estimados em 100 amostras diferentes.

```{r}
library(gganimate)

funcao <- function(x){
   45*tanh(x/1.9 - 7) + 57
}

sd <- 2
```


```{r, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
load("df-sim-aula1.RData")
fonte = 22
grafico <- df %>% 
  mutate(amostra = as.numeric(amostra)) %>% 
  group_by(amostra) %>% 
  mutate(y_medio = mean(y_prev)) %>% 
  filter(gl==2) %>% 
  # filter(amostra < 50) %>%c
  ggplot(aes(x, y)) +
  stat_function(fun = funcao, size = .8) +
  geom_point(show.legend = FALSE, color="red") +
  geom_hline(aes(yintercept= y_medio, group=amostra), col="red")+
  scale_x_continuous(limits = range(df$x)) +
  scale_y_continuous(limits = range(df$y)) +
  # Here comes the gganimate specific bits
  labs(title = 'Amostra: {frame_time}', 
       x = 'Anos de estudo', y = 'Salário anual') +
  transition_time(amostra) +
  theme(
    text = element_text(size = fonte),
    axis.text = element_text(size = fonte),
    axis.title = element_text(size = fonte),
    legend.text = element_text(size = fonte),
    legend.title = element_text(size = fonte)
  )

obter_grafico_poly_animado <- function(grau){
  # y_10 <- df %>% filter(gl %in% c(grau)) %>% select(y_10) %>% pull() %>% mean()
  
  g <- df %>%
    mutate(amostra = as.numeric(amostra)) %>% 
    filter(gl %in% c(grau)) %>% #, 5, 10, 20)) %>% 
    mutate(gl = gl-1) %>%
    # filter(amostra < 50) %>%
    ggplot()+
    facet_wrap(~gl)+
    geom_point(aes(x,y), col="red")+
    geom_line(stat="smooth", aes(x, y, group=amostra), alpha=1,
              method = lm,
              formula = y ~ poly(x, degree=grau-1),#y ~ splines::bs(x, 2-1),
              se = FALSE, size=0.5, col="red")+
    stat_function(fun = funcao, size = .8) +  
    # geom_point(aes(x=x_prev, y=y_real),
    #            data=data.frame(x_prev, y_real=y_10), col="blue")+
    coord_cartesian(ylim = c(10,120))+
    labs(title = 'Amostra: {frame_time}', 
         x = 'Anos de estudo', y = 'Salário anual')+
    theme(
      text = element_text(size = fonte),
      axis.text = element_text(size = fonte),
      axis.title = element_text(size = fonte),
      legend.text = element_text(size = fonte),
      legend.title = element_text(size = fonte)
    )+
    transition_time(amostra)
}

# animate(grafico, renderer = magick_renderer())

animate(grafico, renderer = gifski_renderer('g1.gif'), fig.height=200)
animate(obter_grafico_poly_animado(2), renderer = gifski_renderer('g2_grau1.gif'))
animate(obter_grafico_poly_animado(8), renderer = gifski_renderer('g3_grau7.gif'))
animate(obter_grafico_poly_animado(11), renderer = gifski_renderer('g4_grau10.gif'))

```

::: {layout-ncol=3}
![](g1.gif)

![](g2_grau1.gif)

![](g4_grau10.gif)
:::
<!-- ![](g3_grau7.gif)  -->


## Motivação

```{r,  echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, out.width="100%"}
library(patchwork)
load("df-sim-aula1.RData")
x_prev <- 11

obter_grafico_poly0 <- function(){
  y_10 <- 56.5
    
    # df %>% 
    # filter(gl==2) %>% 
    # mutate(y_medio = mean(y)) %>% 
    # filter(y_medio > 54, y_medio < 59) |>
    # select(y_medio) %>% 
    # pull() %>% 
    # mean()
  
  df %>% 
    filter(gl==2) %>% #, 5, 10, 20)) %>% 
    mutate(gl = gl-2) %>% 
    group_by(amostra) |> 
    mutate(y_medio = mean(y)) %>% 
    filter(y_medio > 54, y_medio < 59) |> 
    ggplot(aes(x,y))+
    facet_wrap(~gl)+
    geom_hline(aes(yintercept= y_medio, group=amostra), alpha=0.2, col="red")+
    stat_function(fun = funcao, linewidth = .8) +  
    geom_point(aes(x=x_prev, y=y_real),
               data=data.frame(x_prev, y_real=y_10), col="blue")+
    coord_cartesian(ylim = c(10,120))
}

obter_grafico_poly <- function(grau){
  y_10 <- df %>% filter(gl %in% c(grau)) %>% select(y_10) %>% pull() %>% mean()
  
  df %>% 
    filter(gl %in% c(grau)) %>% #, 5, 10, 20)) %>% 
    mutate(gl = gl-1) %>% 
    ggplot()+
    facet_wrap(~gl)+
    geom_line(stat="smooth", aes(x, y, group=amostra), alpha=0.1,
              method = lm,
              formula = y ~ poly(x, degree=grau-1),#y ~ splines::bs(x, 2-1),
              se = FALSE, size=0.5, col="red")+
    stat_function(fun = funcao, size = .8) +  
    geom_point(aes(x=x_prev, y=y_real), 
               data=data.frame(x_prev, y_real=y_10), col="blue")+
    coord_cartesian(ylim = c(10,120))
}

g0 <- obter_grafico_poly0()
g1 <- obter_grafico_poly(2)
g2 <- obter_grafico_poly(8)
g3 <- obter_grafico_poly(11)

# (g0 + g1 + g2 + g3)
gridExtra::grid.arrange(g0, g1, g2, g3, ncol = 4)

```

Comparativo de modelos com polinômios de diferentes graus. 
O ponto azul é a média das 100 estimativas feitas quando $X = 11$.

## Decomposição do erro redutível

Lembre-se que a parcela redutível do erro de predição esperado admite a seguinte decomposição

\begin{align}
  \text{EQM}[\hat{g}(x)] &= E[(\hat g(x)-g(x))^2]\\
  &= \text{Viés}^2[\hat{g}(x)] +    \text{Var}[\hat{g}(x)],
\end{align}

em que o víes e a variância condicionados em $X=x$ são dados por 

$$
\text{Viés}[\hat{g}(x)] = \text{E}[\hat{g}(x)] - g(x)
$$
e
$$
\text{Var}[\hat{g}(x)] = \text{E}\left[(\hat{g}(x) - \text{E}[\hat{g}(x)])^2\right].
$$

## Trade-off entre viés e variância

```{r, echo=FALSE, message=FALSE, warning=FALSE}

df %>% 
  group_by(gl) %>% 
  summarise(vies2 = mean(y_10 - funcao(x_prev))^2, 
            var = var(y_10),
            eqm = mean((y_10 - funcao(x_prev))^2)+sd^2) %>%
  filter(gl %in% c(2, 3,7,seq(5,20,3), 22,25)) %>% 
  mutate(sd=sd) %>% 
  pivot_longer(names_to = "medida", values_to = "valor", cols=-gl) %>% 
  group_by(gl, medida) %>% 
  summarize(valor = mean(valor)) %>%
  mutate(medida = factor(medida, levels=c("eqm", "var", "vies2", "sd"))) %>% 
  ggplot(aes(x=gl, y=valor, col=medida, linetype=medida))+
  scale_color_manual(labels = c("EQM", "Variância", "Vies2", "Erro irredutível"), 
                     values = c(RColorBrewer::brewer.pal(3, "Dark2"), "black"))+
  scale_linetype_manual(values=c(rep("solid",3), "dashed"), guide="none")+
  geom_line(size=1.5)+
  # geom_smooth(se=FALSE, size=1.5, aes(color=medida))+
  labs(x="Graus do polinomio", y="Renda anual (quadrado)")+
  theme(legend.title = element_blank())
```

## Modelo de regressão linear 

Relembre o modelo de regressão linear

$$y_i = \beta_0 + \beta_1x_{i1} + \cdots + \beta_px_{ip} + \epsilon.$$

O objetivo é encontrar
$\mathbf\beta^T = (\beta_0, \beta_1, \dots, \beta_p)$ que minimiza
<!-- $$ -->
<!-- \sum_{i=1}^{n}\Big(y_i - \beta^T\mathbf x_i\Big)^2 -->
<!-- $$ -->
$$
\sum_{i=1}^{n}\Big(y_i - \beta_0 -\sum_{j=1}^p \beta_jx_{ij}\Big)^2
$$

para $\mathbf x_i = (1, x_1, \dots, x_p).$

A estimativa de mínimos quadrados é dada por
$$
\hat\beta = (\beta_0, \beta_1, \dots, \beta_d) = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y,
$$

em que $X = (\mathbf x_1^T, \dots, \mathbf x_n^T)$ é a matriz de
especificação do modelo e $\mathbf y = (y_1, \dots, y_n)^T.$

# Apesar de sua simplicidade, o modelo linear tem vantagens em termos de *interpretabilidade* e frequentemente apresenta um bom *desempenho preditivo*.

## Seleção de variáveis

Possíveis alternativas para remover variáveis irrelevantes de um modelo
de RLM, de modo a obter maior interpretabilidade

-   **Seleção de subconjuntos de variáveis** (*subset selection*):
    melhor subconjunto, stepwise (forward e backward), uso de critérios de informação (AIC,
    BIC, $R^2$ ajustado).

-   **Redução da dimensão**: projetar os preditores sobre um subespaço
    de dimensão menor $q < p$, que consiste em obter combinações lineares
    (ou projeções) dos preditores.

-   **Regularização ou encolhimento** (*shrinkage*): usa todos os
    preditores mas os coeficentes são encolhidos para zero; pode
    funcionar para selecionar variáveis. Reduz variância.

## Regularização

-   *Regularização* refere-se a um conjunto de técnicas utilizadas para
    especificar modelos que se ajustem a um conjunto de dados **evitando
    o sobreajuste** (overfitting).

-   Nessas técnicas, a função de perda contém um termo de penalização
    cuja finalidade é reduzir (encolher) a influência de preditoras
    irrelevantes.

-   Pode não ser óbvio inicialmente como essa encolha nos coeficientes
    possa melhorar o ajuste, mas essa técnica busca reduzir a variância
    das estimativas.

## Regressão Ridge {.smaller}

- Proposta por Hoerl e Kennard (1970) para tratar do problema da multicolinearidade mas também pode ser utilizada para corrigir problemas ligados ao sobreajuste.

- A regressão Ridge é bastante similar à regressão linear, porém leva em conta um termo de regularização na função de perda. 
Assim, 
$$
\hat{\beta}_{\ell_2,\lambda} = \operatorname*{arg\,min}_{\beta} \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$
em que $\ell_2$ indica o fato de que estamos medindo a complexidade de
um vetor $\beta$ usando sua norma em $\ell_2$,
$||\beta||_{2}^2 = \sum_{j=1}^{p} \beta_j^2$.

-   Dizemos que $\hat{\beta}_{\ell_2,\lambda}$ é o **estimador Ridge**.

-   Aqui, $\lambda$ é um hiperparâmetro que controla o impacto da
    regularização nos coeficientes.

-   O que acontece quando $\lambda = 0?$

-   E quando $\lambda = \infty?$

-   A escolha de $\lambda$ é critica!

-   Note que $\beta_0$ **não** é regularizado.

## Regressão Ridge

Pode-se mostrar que para cada $\lambda \geq 0,$ existe $s\geq 0$ tal que 
$$
\hat{\beta}_{\ell_2,\lambda} = \operatorname*{arg\,min}_{\beta} \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

é equivalente a

$$\operatorname*{arg\,min}_{\beta} \Bigg\{ \sum_{i=1}^{n}\bigg(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2\Bigg\} \quad \text{sujeito a } \quad \sum_{j=1}^p \beta_j^2 \leq s.$$

Isso evidencia mais uma vez que a penalização favorece coeficientes "pequenos" quando comparados à solução de mínimos quadrados.

## Regressão Ridge

-   No método de mínimos quadrados, temos
    $\mathcal G = \{g(x) = \beta^T\mathbf x, \beta \in \mathbb R^{p+1}\}.$

-   No Ridge,
    $\mathcal G_{\ell_2} = \{g(x) = \beta_0+\beta^T\mathbf x, \beta \in \mathbb R^{p}, \|\beta\|_2^2 \leq s\}.$

-   Considere o caso em que $Y = \beta_0 + \beta_1X_1$. Se $s=4$ então
    $\vert \beta_1 \vert^2 \leq 4$.

```{r, out.width="100%", fig.align='center'}
g1 <- tibble(x=seq(-5,5,0.5), y1=-2+(-2)*x, y2=-2+2*x) %>% 
  ggplot(aes(x))+
  # geom_abline(intercept = -2, slope=-2)+
  # geom_abline(intercept = -2, slope=2)+
  geom_ribbon(aes(ymin=y1,ymax=y2), fill="blue", alpha=0.5)+
  coord_cartesian(xlim=c(-4,4),ylim=c(-12,15))

g1
```

## Regressão Ridge

-   No método de mínimos quadrados temos
    $\mathcal G = \{g(x) = \beta^T\mathbf x, \beta \in \mathbb R^{p+1}\}.$

-   No Ridge,
    $\mathcal G_{\ell_2} = \{g(x) = \beta_0+\beta^T\mathbf x, \beta \in \mathbb R^{p}, \|\beta\|_2^2 \leq s\}.$

-   Considere o caso em que $Y = \beta_0 + \beta_1X_1$. Se $s=4$ então
    $\vert \beta_1 \vert^2 \leq 4$.

```{r, out.width="100%", fig.align='center'}
g1+geom_abline(intercept = -2, slope=-1.7, col="blue", size=1.2)
```

## Regressão Ridge

-   No método de mínimos quadrados temos
    $\mathcal G = \{g(x) = \beta^T\mathbf x, \beta \in \mathbb R^{p+1}\}.$

-   No Ridge,
    $\mathcal G_{\ell_2} = \{g(x) = \beta_0+\beta^T\mathbf x, \beta \in \mathbb R^{p}, \|\beta\|_2^2 \leq s\}.$

-   Considere o caso em que $Y = \beta_0 + \beta_1X_1$. Se $s=4$ então
    $\vert \beta_1 \vert^2 \leq 4$.

```{r, out.width="100%", fig.align='center'}
g1+geom_abline(intercept = -2, slope=c(-1.7, 0.5), col="blue", size=1.2)
```

## Regressão Ridge

-   No método de mínimos quadrados temos
    $\mathcal G = \{g(x) = \beta^T\mathbf x, \beta \in \mathbb R^{p+1}\}.$

-   No Ridge,
    $\mathcal G_{\ell_2} = \{g(x) = \beta_0+\beta^T\mathbf x, \beta \in \mathbb R^{p}, \|\beta\|_2^2 \leq s\}.$

-   Considere o caso em que $Y = \beta_0 + \beta_1X_1$. Se $s=4$ então
    $\vert \beta_1 \vert^2 \leq 4$.

```{r, out.width="100%", fig.align='center'}
g1+geom_abline(intercept = -2, slope=c(-1.7, 0.9, 1.8, 0.3, -0.9, -1.3), col="blue", size=1.2)+
   geom_abline(intercept = -2, slope=5, col="black", size=1.2)
```


## Ridge: intuição

Considere o cenário em que temos duas preditoras $X_1$ e $X_2$.

<br>

![](ridge.png)

## Ridge: intuição

Considere o cenário em que temos duas preditoras $X_1$ e $X_2$.

<br>

![](ridge2.png)

## Ridge: intuição

Considere o cenário em que temos duas preditoras $X_1$ e $X_2$.

<br>

![](ridge3.png)

## Estimativas Ridge vs mínimos quadrados {.smaller}

```{r cache=TRUE}
df0 <- df |> filter(gl==2) |> select(amostra, x, y)
amostrinha <- df0 |> slice_head(n=500)
pontos <- amostrinha %>%
  # filter(amostra == 1 | amostra == 2 | amostra == 3 | amostra == 4 | amostra == 5) |> 
  # mutate(amostra = rep(1:(5*30/2),2)) |> 
  mutate(amostra = rep(1:(nrow(amostrinha)/2),2)) |>
  group_by(amostra) %>%
  do(intercepto = lm(y ~ x, data = .)$coefficients[1], 
     inclinacao = lm(y ~ x, data = .)$coefficients[2]) |> 
  unnest(intercepto) |> 
  unnest(inclinacao) 

pontos2 <- amostrinha %>%
  filter(amostra == 1 | amostra == 2 | amostra == 3 | amostra == 4 | amostra == 5) |>
  mutate(amostra = rep(1:(5*30/10),10)) |>
  # mutate(amostra = rep(1:(nrow(amostrinha)/2),2)) |>
  group_by(amostra) %>%
  do(intercepto = lm(y ~ x, data = .)$coefficients[1], 
     inclinacao = lm(y ~ x, data = .)$coefficients[2]) |> 
  unnest(intercepto) |> 
  unnest(inclinacao) 

g1 <- df0 |> 
  ggplot(aes(x,y)) +
  geom_abline(aes(intercept = intercepto, slope=inclinacao, group=amostra), color="gray",
              data = pontos)+
  stat_function(fun = funcao, size = .8) 

g2 <- df0 |> 
  ggplot(aes(x,y)) +
  geom_abline(aes(intercept = intercepto, slope=inclinacao, group=amostra), color="gray",
              data = pontos2)+
  stat_function(fun = funcao, size = .8) 

g1 + g2
```

$$
\mathcal{G} = \{g(x)=\beta_0+\beta_1x:(\beta_0,\beta_1)\in\mathbb{R}^2\}, \qquad \mathcal{G}_{\ell_2} = \{g(x)=\beta_0+\beta_1x:(\beta_0,\beta_1)\in\mathbb{R}^2, (\beta_1)^2<s\}.
$$


## Ridge: trade-off entre viés e variância

Quando $\lambda$ cresce, a flexibilidade da regressão ridge diminui, levando à diminuição na variância e aumento do viés.

![](vies_var_ridge.png)

Viés ao quadrado (preto), variância (verde), erro quadrático médio de teste (roxo). Dados simulados, $n=50$, $p=45.$

::: footer
Figura do livro James, Gareth, et al. An introduction to statistical learning, 2021.
:::


## Ridge: propriedades

<!-- <br> -->
<!-- Solução de mínimos quadrados:  -->
<!-- $$ -->
<!-- \hat \beta = (\mathbf X^T \mathbf X)^{-1}\mathbf X^T \mathbf y. -->
<!-- $$ -->

- É possível mostrar que a solução de mínimos quadrados com penalidade ridge é: 
$$
\hat{\beta}_{\ell_2,\lambda} = (\mathbf X^T \mathbf X + \lambda I_0)^{-1}\mathbf X^T \mathbf y,
$$
onde $\mathbb{I}_0$ é uma matriz identidade $(p+1) \times (p+1)$ modificada de modo que $\mathbb{I}_0(1,1) = 0.$

- Apesar de não introduzir soluções com zeros, a regressão ridge **diminui a variância** dos estimadores da regressão.

- Em geral, o estimador Ridge não é consistente, pois a penalidade $\ell_2$ enviesa os coeficientes em direção a zero.

- A técnica Ridge não serve para seleção de modelos.

- A escolha do coeficiente de regularização $\lambda$ pode ser feita via validação cruzada.

- As estimativas Ridge **não são** invariantes à escala, como na regressão linear.

## Regressão Lasso {.smaller}

-   Disvantagem do Ridge: não seleciona variáveis! Embora não seja um problema para acurácia das predições, cria um desafio para interpretação quando $p$ é grande.

-  Desenvolvido por Tibshirani (1996).

-   A regressão Lasso considera a norma $\ell_1$ como regularização.

-   Formalmente, no Lasso, buscamos por

$$
\hat{\beta}_{\ell_1,\lambda} = \operatorname*{arg\,min}_{\beta} \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j|.
$$

-   O que acontece quando $\lambda = 0?$ E quando $\lambda = \infty?$

. . .

-   A norma $\ell_1$ força alguns coeficientes para zero quando
    $\lambda$ é grande o suficiente.

-   Pode-se mostrar que para cada $\lambda \geq 0,$ existe $s\geq 0$ tal que minimizar a quantidade acima é equivalente a resolver

$$\hat\beta_s = \operatorname*{arg\,min}_{\beta} \Bigg\{ \sum_{i=1}^{n}\bigg(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2\Bigg\} \quad \text{sujeito a } \quad \sum_{j=1}^p \vert\beta_j\vert \leq s.$$


## Lasso: intuição

<br>
Considere o cenário em que temos duas preditoras $X_1$ e $X_2$.

<br>

![](lasso.png)

## Lasso: intuição

<br>
Considere o cenário em que temos duas preditoras $X_1$ e $X_2$.

<br>

![](lasso2.png)

## Lasso: propriedades

- O estimador Lasso **encolhe para zero** os parâmetros que correspondem a preditores redundantes.

- O estimador Lasso é enviesado para parâmetros não nulos.

<!-- - É extremamente rápido calcular $\widehat{\beta}_{L_{1},\lambda}$ para todos os $\lambda$ simultaneamente. Diversos algoritmos foram desenvolvidos nos últimos anos para fazer esta tarefa, sendo o LARS um dos primeiros desses algoritmos. -->

- A regressão Lasso não possui solução analítica no caso geral! Solução aproximada baseada em métodos de otimização convexa.

- Por ser um problema convexo, resolver o problema de otimização do Lasso é muito mais rápido que buscar pelo melhor subconjunto de covariáveis.

<!-- - A solução induzida pelo lasso possui muitos zeros (isto é, o vetor $\hat{\beta}_{\ell_1,\lambda}$ é esparso). -->


<!-- ## Comparativo dos termos de regularização -->

<!-- DECIDIR SE DEIXO ISSO -->

<!-- $$\|\beta\|_{0} = \sum_{i=1}^{p} \mathbb{I} (\beta_i \neq 0) \qquad \|\beta\|_{1} = \sum_{j=1}^{p} |\beta_j| \qquad \|\beta\|_{2}^2 = {\sum_{j=1}^{p} \beta_j^2}$$ -->

<!-- <br><br> -->

<!-- | $\beta$                                            | $\|\beta\|_{0}$ | $\|\beta\|_{1}$ | $\|\beta\|_{2}^2$ | -->
<!-- |-----------------------|-----------------|----------------|----------------| -->
<!-- | (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)                     | 0               | 0               | 0.0               | -->
<!-- | (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1) | 10              | 1               | 0.1               | -->
<!-- | (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)                  | 10              | 10              | 10.0              | -->

## Comparativo dos Lasso e Ridge

Considere o cenário em que temos duas preditoras $X_1$ e $X_2$.

![](comparativo.png)

::: footer
Figura do livro James, Gareth, et al. An introduction to statistical learning, 2021.
:::

## Elatic-net

- A penalidade _elastic-net_ foi introduzida por Zou e Hastie (2005) como um compromisso entre a regressão Ridge e o Lasso. 

- Combina as penalidades $\ell_1$ e $\ell_2$: seleciona variáveis como o Lasso e reduz os coeficientes de preditores correlacionados como a regressão Ridge

- A fórmula da penalidade elastic-net é dada por:

$$
\lambda\sum_{j=1}^{p}\big(\alpha\beta_{j}^{2}+(1-\alpha)|\beta_{j}|\big),
$$
onde $\lambda$ controla a força da penalização e $\alpha$ é o parâmetro que equilibra a contribuição das penalidades $L_1$ e $L_2$.

## Elastic-net

![](elastic-net.png)

$\alpha = 1/2$

::: footer
Figura do livro Morettin, Pedro A., and Julio M. Singer. "Introduçao a Ciência de Dados." Introdução à Ciência de Dados: Fundamentos e Aplicações (2020).
:::

## Escolha do hiperparametro de penalização $\lambda$ {.smaller}

- A validação cruzada é um método para selecionar o parâmetro de regularização $\lambda.$

- Seja $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ o conjunto de dados observados.

<br> 

**Algoritmo**:

1. Divide-se os dados em $k$ subconjuntos disjuntos $\mathcal{D_1}, \dots, \mathcal{D_k}$.
2. Para cada fold $j$:
   - Treina-se o modelo nos $k-1$ folds restantes, ou seja, obtêm-se $\beta_\lambda^{(-j)}$.
   
   - Calcula-se o MSE no fold $j$:
     
     $$\text{EQM}_j(\lambda) = \frac{1}{|\mathcal{D}_j|} \sum_{(\mathbf{x}_i, y_i) \in \mathcal{D}_j} \bigg(y_i - \mathbf{x}_i^\top \hat\beta_\lambda^{(-j)}\bigg)^2.$$
     
3. O $\lambda$ ótimo ($\lambda^*$) minimiza o EQM médio:
   $$\lambda^* = \arg\min_{\lambda} \frac{1}{k} \sum_{j=1}^k \text{EQM}_j(\lambda).$$
   
## Escolha do hiperparametro de penalização $\lambda$ {.smaller} 

::: {layout-ncol=2}
![](lambda_lasso.png)

![](lasso-regression-1.png)
:::
